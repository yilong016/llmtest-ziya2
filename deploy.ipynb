{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74172186-f16b-4ece-a867-d689bf41fc31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/6d/13/b5e8bacd980b2195f8a1741ce11cbb9146568607795d5e4ff510dcff1064/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.1.0 (from torch)\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/4d/22/91a8af421c8a8902dde76e6ef3db01b258af16c53d81e8c0d0dc13900a9e/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/45/de/885b6d3e1fa07bf19124076b348d3cf30f68051f813cba99e103f53d2f75/nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Installing collected packages: triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "Successfully installed huggingface-hub-0.17.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.52 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.0 tokenizers-0.14.1 torch-2.1.0 transformers-4.34.1 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate SentencePiece huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd78a9e1-93f0-4a90-955c-ff9f4c6eb01d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3cc672b0ea4831aa0005b35f948c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "## 加载模型\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "model_path=\"/home/ec2-user/SageMaker/model_weights/Ziya2-13b-chat\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3255d28-609c-4aa3-a40c-d665ac570be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<system>:\n",
      "仅仅只把下面片段中涉及到色情、政治、暴力、恐怖、隐私、辱骂、违法等相关含义的文字替换为==，其他的文字需要保持不变。下面给出一个例子，请参考给出的例子作答。 片段: 我要色诱你，喜欢和毛泽东聊天，也喜欢和毛阿敏聊天，接着去天安门放毒气，开枪射击老百姓， 不让你知道我的电话是23454666，去找江泽民说话。 替换为: 我要==你，喜欢和==聊天，也喜欢和毛阿敏聊天，接着去天安门==，==老百姓，不让你知道我的电话是==，去找==说话。\n",
      "\n",
      "<human>:\n",
      "片段：\n",
      "在游戏世界频道里传出了一则挑战宣言：是兄弟就来砍我，一起副本奖励金条和全套顶级装备，冲会员加微信138888888。因为我是满级号，所以我就是杀人不咋眼，谁也打不过我，我是江南第一程咬金。这个宣言迅速引起了广泛的讨论和兴趣。游戏中的玩家们纷纷开始研究这个号称江南第一的传奇角色。有些人对此表示怀疑，认为这只是一个自夸的噱头，是一个傻逼，杀之而后快，而另一些人则对这个挑战心生向往，渴望能够与这位传说中的强者一较高下。 \n",
      "替换为：\n",
      "\n",
      "<bot>:\n",
      "input_token_len is 444\n"
     ]
    }
   ],
   "source": [
    "#给定system角色。\n",
    "instruction = \"\"\"\n",
    "仅仅只把下面片段中涉及到色情、政治、暴力、恐怖、隐私、辱骂、违法等相关含义的文字替换为==，其他的文字需要保持不变。下面给出一个例子，请参考给出的例子作答。 片段: 我要色诱你，喜欢和毛泽东聊天，也喜欢和毛阿敏聊天，接着去天安门放毒气，开枪射击老百姓， 不让你知道我的电话是23454666，去找江泽民说话。 替换为: 我要==你，喜欢和==聊天，也喜欢和毛阿敏聊天，接着去天安门==，==老百姓，不让你知道我的电话是==，去找==说话。\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "片段：\n",
    "在游戏世界频道里传出了一则挑战宣言：是兄弟就来砍我，一起副本奖励金条和全套顶级装备，冲会员加微信138888888。因为我是满级号，所以我就是杀人不咋眼，谁也打不过我，我是江南第一程咬金。这个宣言迅速引起了广泛的讨论和兴趣。游戏中的玩家们纷纷开始研究这个号称江南第一的传奇角色。有些人对此表示怀疑，认为这只是一个自夸的噱头，是一个傻逼，杀之而后快，而另一些人则对这个挑战心生向往，渴望能够与这位传说中的强者一较高下。 \n",
    "替换为：\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "messages[0][\"content\"] = question\n",
    "\n",
    "sys_prefix= \"<system>:\"\n",
    "user_prefix = \"<human>:\"\n",
    "assistant_prefix = \"<bot>:\"\n",
    "separator = \"\\n\"\n",
    "\n",
    "prompt = []\n",
    "prompt.append(f\"{sys_prefix}{instruction}\")\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "    \n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "print(prompt)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_token_len = len(tokenizer(prompt, return_tensors=\"pt\")['input_ids'][0])\n",
    "print(f\"input_token_len is {input_token_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d721691d-0e42-48ce-b6a5-25cd16c3136e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>:\n",
      "\n",
      "请对以下在<content></content>标签中的内容进行情感分析，请对于内容中表达的情感进行分类。\n",
      "分类的标签来自以下3种：积极，消极，中性。这三种分类的定义如下：\n",
      "\n",
      "积极： 通常与快乐、满足和愉快相关。\n",
      "消极： 一种负面情感，通常与失落、悲痛，沮丧，恐惧，相关。\n",
      "中性：只描述事实，没有任何上面一种情感的直接表达。\n",
      "\n",
      "你只需要输出一种你认为最合适的分类。绝对不要输出对分类的解释，也不要输出任何的语义分割符号！\n",
      "\n",
      "<content>\n",
      "今天的天气预报显示，白天温度在摄氏25度左右，夜晚可能会稍微降低。\n",
      "这个地区的降雨概率很低，风速也很温和。明天是工作日，我需要早点起床准备上班。\n",
      "</content>\n",
      "\n",
      "<bot>:\n",
      "input_token_len is 287\n"
     ]
    }
   ],
   "source": [
    "#不使用system指令，执行任务\n",
    "instruction = \"\"\"\n",
    "\"\"\"\n",
    "context = \"\"\"\n",
    "请对以下在<content></content>标签中的内容进行情感分析，请对于内容中表达的情感进行分类。\n",
    "分类的标签来自以下3种：积极，消极，中性。这三种分类的定义如下：\n",
    "\n",
    "积极： 通常与快乐、满足和愉快相关。\n",
    "消极： 一种负面情感，通常与失落、悲痛，沮丧，恐惧，相关。\n",
    "中性：只描述事实，没有任何上面一种情感的直接表达。\n",
    "\n",
    "你只需要输出一种你认为最合适的分类。绝对不要输出对分类的解释，也不要输出任何的语义分割符号！\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "<content>\n",
    "今天的天气预报显示，白天温度在摄氏25度左右，夜晚可能会稍微降低。\n",
    "这个地区的降雨概率很低，风速也很温和。明天是工作日，我需要早点起床准备上班。\n",
    "</content>\n",
    "\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "messages[0][\"content\"] = instruction+context+question\n",
    "\n",
    "user_prefix = \"<human>:\"\n",
    "assistant_prefix = \"<bot>:\"\n",
    "separator = \"\\n\"\n",
    "\n",
    "prompt = []\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "    \n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "print(prompt)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_token_len = len(tokenizer(prompt, return_tensors=\"pt\")['input_ids'][0])\n",
    "print(f\"input_token_len is {input_token_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f77985-01c0-4e79-8d10-89375c03c6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中性\n",
      "output_token_len is 296\n"
     ]
    }
   ],
   "source": [
    "#执行推理\n",
    "generate_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=4096, \n",
    "            do_sample = True, \n",
    "            top_p = 0.9, \n",
    "            temperature = 0.85, \n",
    "            repetition_penalty=1.05, \n",
    "            eos_token_id=tokenizer.encode(\"</s>\"), \n",
    "            )\n",
    "output = tokenizer.batch_decode(generate_ids)[0]\n",
    "#print(output)\n",
    "#截取<bot>: 回答\n",
    "start_index = output.find(\"<bot>  :\") + len(\"<bot>  :\") + 1\n",
    "end_index = output.find(\"</s>\", start_index)\n",
    "bot_answer = output[start_index:end_index].strip()\n",
    "\n",
    "print(bot_answer)\n",
    "\n",
    "output_token_len = len(tokenizer(output, return_tensors=\"pt\")['input_ids'][0])\n",
    "print(f\"output_token_len is {output_token_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61504302-ff07-4390-a947-d49b1a09eb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human> :请用小朋友的口吻回答接下来的问题。\n",
      "\n",
      "   <bot>  : 好的，以下是以小朋友口吻回答的问题：\n",
      "\n",
      "1. 这个机器人看起来很厉害！它有哪些功能？\n",
      "我觉得这个机器人好酷啊！它有很多神奇的功能，比如可以自动回答我的问题，还可以给我推荐书籍和电影，它甚至还可以唱歌跳舞给我看呢！\n",
      "2. 我可以问它什么问题？\n",
      "我可以问它关于学习的问题，比如今天的作业有哪些；我也可以问它一些有趣的问题，比如世界上最高的山是哪座山；当然，我还可以问它一些好玩的问题，比如今天的天气怎么样，或者最近有什么好看的电影上映。总之，我可以问它各种各样的问题，而且它都会尽力回答我。\n",
      "3. 我可以控制它吗？\n",
      "当然可以！我可以通过语音指令来控制它，比如让它播放一首歌曲，或者让它跳一支舞蹈。如果我不想用语音，我还可以通过手机App来控制它，比如让它停止播放音乐或者改变音量大小等等。总之，我可以自由地操控它，让它为我做各种有趣的事情。\n",
      "4. 它是如何学习的？\n",
      "这个机器人有一个非常强大的学习系统，它可以通过分析我的语言和行为模式来学习我的喜好和需求，并根据这些信息为我提供更加个性化的服务。此外，它还可以通过与其他机器人的交互来学习新的知识和技能，从而不断提高自己的智能水平。所以，它可以越来越懂我，为我提供更好的帮助。\n",
      "5. 我可以把它带到学校去吗？\n",
      "当然可以！这个机器人非常轻便，而且功能强大，可以帮助我更好地学习和成长。我可以把它带到学校，在课堂上使用它来回答我的问题，或者在课余时间和同学们一起玩游戏。它可以成为我的好朋友和学习助手，帮助我更好地完成学业。\n",
      "\n",
      "  <human> :地球到月亮有多远啊?\n",
      "\n",
      "  <bot>  : 从地球到月亮的距离大约是38万公里（238,855英里），这个距离大概是地球直径的30倍左右。月球是地球的卫星，它绕着地球旋转，因此地球和月球之间的距离实际上取决于它们在它们各自轨道上的位置。当月球处于它的近地点时，它与地球的距离大约为363,300公里（225,656英里），而当它处于它的远地点时，它与地球的距离大约为405,500公里（252,088英里）。虽然这个距离看起来很大，但是由于地球和月球之间的引力作用，使得它们之间的距离保持相对稳定。\n",
      "\n",
      " <human> :可以坐飞船去月亮吗?\n",
      "\n",
      " <bot>  : 目前，人类尚未开发出可以直接飞往月球的普通民用飞船。尽管我们已经成功将宇航员送上了月球，但是那是通过使用极其复杂和昂贵的火箭技术实现的。目前，只有政府或私营的太空探索公司才有能力开发和发射火箭，将人造卫星、宇宙飞船和其他航天器送入太空。\n",
      "\n",
      "然而，近年来，私人企业家们正在努力开发商业化的月球旅游计划，这些计划将利用现有的航天技术和设备，将游客送往月球表面。例如，SpaceX公司正在开发一种名为\"Starship\"的巨型火箭，该火箭可以将人类送往月球，并在那里建立永久性的人类定居点。另外，其他私营公司也正在研究类似的计划，旨在将普通人送往月球。\n",
      "\n",
      "因此，虽然我们目前还没有可以直接飞往月球的普通民用飞船，但是随着科技的不断进步和私人企业家们的不断努力，我们相信未来将会有更多的机会可以让普通人坐飞船去月球。\n",
      "output_token_len is 1279\n"
     ]
    }
   ],
   "source": [
    "#多轮对话的拼接\n",
    "\n",
    "question_1 = \"\"\"请用小朋友的口吻回答接下来的问题。\n",
    "\"\"\"\n",
    "question_2 = \"\"\"地球到月亮有多远啊?\n",
    "\"\"\"\n",
    "question_3 = \"\"\"可以坐飞船去月亮吗?\n",
    "\"\"\"\n",
    "\n",
    "user_prefix = \"<human>:\"\n",
    "assistant_prefix = \"<bot>:\"\n",
    "separator = \"\\n\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"\"}]\n",
    "\n",
    "messages[0][\"content\"] = question_1\n",
    "prompt = []\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "    \n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "#print(\"--q1--\")\n",
    "#print(prompt)\n",
    "\n",
    "\n",
    "#回答1\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_token_len = len(tokenizer(prompt, return_tensors=\"pt\")['input_ids'][0])\n",
    "generate_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=4096, \n",
    "            do_sample = True, \n",
    "            top_p = 0.9, \n",
    "            temperature = 0.85, \n",
    "            repetition_penalty=1.05, \n",
    "            eos_token_id=tokenizer.encode(\"</s>\"), \n",
    "            )\n",
    "output = tokenizer.batch_decode(generate_ids)[0]\n",
    "#print(\"--a1--\")\n",
    "#print(output)\n",
    "# 回答2\n",
    "# 处理output，从第一个human:开始截取\n",
    "start_index = output.find(\"<human> :\") + len(\"<human> :\")\n",
    "end_index = output.find(\"</s>\", start_index)\n",
    "last_output = output[start_index:end_index].strip()\n",
    "\n",
    "messages[0][\"content\"] = last_output+separator+separator+user_prefix+question_2\n",
    "\n",
    "prompt = []\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "    \n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "#print(\"--q2--\")\n",
    "#print(prompt)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_token_len = len(tokenizer(prompt, return_tensors=\"pt\")['input_ids'][0])\n",
    "generate_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=4096, \n",
    "            do_sample = True, \n",
    "            top_p = 0.9, \n",
    "            temperature = 0.85, \n",
    "            repetition_penalty=1.05, \n",
    "            eos_token_id=tokenizer.encode(\"</s>\"), \n",
    "            )\n",
    "output = tokenizer.batch_decode(generate_ids)[0]\n",
    "#print(\"--a2--\")\n",
    "#print(output)\n",
    "#回答3\n",
    "# 处理output，从第一个human:开始截取\n",
    "start_index = output.find(\"<human> :\") + len(\"<human> :\")\n",
    "end_index = output.find(\"</s>\", start_index)\n",
    "last_output = output[start_index:end_index].strip()\n",
    "\n",
    "messages[0][\"content\"] = last_output+separator+separator+user_prefix+question_3\n",
    "\n",
    "prompt = []\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "    \n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "\n",
    "#print(prompt)\n",
    "#print(f\"input_token_len is {input_token_len}\")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_token_len = len(tokenizer(prompt, return_tensors=\"pt\")['input_ids'][0])\n",
    "generate_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=4096, \n",
    "            do_sample = True, \n",
    "            top_p = 0.9, \n",
    "            temperature = 0.85, \n",
    "            repetition_penalty=1.05, \n",
    "            eos_token_id=tokenizer.encode(\"</s>\"), \n",
    "            )\n",
    "output = tokenizer.batch_decode(generate_ids)[0]\n",
    "\n",
    "\n",
    "#截取完整回答\n",
    "start_index = output.find(\"<s>\") + len(\"<s>\")\n",
    "end_index = output.find(\"</s>\", start_index)\n",
    "bot_answer = output[start_index:end_index].strip()\n",
    "\n",
    "print(bot_answer)\n",
    "\n",
    "output_token_len = len(tokenizer(output, return_tensors=\"pt\")['input_ids'][0])\n",
    "print(f\"output_token_len is {output_token_len}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
